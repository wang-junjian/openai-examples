{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"content\": \"The 2020 World Series was played at Globe Life Field in Arlington, Texas.\",\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1682651514,\n",
      "  \"id\": \"chatcmpl-7A8oUy0JXdEqufn4rduD8jEAe4uAY\",\n",
      "  \"model\": \"gpt-3.5-turbo-0301\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 17,\n",
      "    \"prompt_tokens\": 57,\n",
      "    \"total_tokens\": 74\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The 2020 World Series was played at Globe Life Field in Arlington, Texas.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"content\": \"Hello!\",\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1682668292,\n",
      "  \"id\": \"chatcmpl-7ADB6cQVKVO1hbwyG2oaxQ1iTP1RD\",\n",
      "  \"model\": \"gpt-3.5-turbo-0301\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 2,\n",
      "    \"prompt_tokens\": 44,\n",
      "    \"total_tokens\": 46\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "        {\"role\": \"system\", \"content\": \"您是个翻译员，帮助用户把中文翻译成英文。\"},\n",
    "        {\"role\": \"user\", \"content\": \"你好！\"},\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 中文翻译英文的机器人"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conversation:\n",
    "    def __init__(self, prompt):\n",
    "        self.prompt = prompt\n",
    "        self.messages = [{\"role\": \"system\", \"content\": self.prompt}]\n",
    "    \n",
    "    def ask(self, question):\n",
    "        self.messages.append({\"role\": \"user\", \"content\": question})\n",
    "\n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=self.messages\n",
    "            )\n",
    "        except Exception as e:\n",
    "            return e\n",
    "        \n",
    "        assistant_message = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
    "\n",
    "        return assistant_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: 你好！\n",
      "Assistant: Hello!\n",
      "\n",
      "User: 你在哪里？\n",
      "Assistant: Where are you located?\n",
      "\n",
      "User: 你在干什么？\n",
      "Assistant: What are you doing?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def translate(conv, input):\n",
    "    output = conv.ask(input)\n",
    "    print(f\"User: {input}\")\n",
    "    print(f\"Assistant: {output}\")\n",
    "    print()\n",
    "\n",
    "conv = Conversation(\"您是个翻译员，帮助用户把中文翻译成英文。\")\n",
    "\n",
    "translate(conv, \"你好！\")\n",
    "translate(conv, \"你在哪里？\")\n",
    "translate(conv, \"你在干什么？\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算 Chat API 调用的 Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\"):\n",
    "    \"\"\"Returns the number of tokens used by a list of messages.\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        print(\"Warning: model not found. Using cl100k_base encoding.\")\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    if model == \"gpt-3.5-turbo\":\n",
    "        print(\"Warning: gpt-3.5-turbo may change over time. Returning num tokens assuming gpt-3.5-turbo-0301.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\")\n",
    "    elif model == \"gpt-4\":\n",
    "        print(\"Warning: gpt-4 may change over time. Returning num tokens assuming gpt-4-0314.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-4-0314\")\n",
    "    elif model == \"gpt-3.5-turbo-0301\":\n",
    "        tokens_per_message = 4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n",
    "        tokens_per_name = -1  # if there's a name, the role is omitted\n",
    "    elif model == \"gpt-4-0314\":\n",
    "        tokens_per_message = 3\n",
    "        tokens_per_name = 1\n",
    "    else:\n",
    "        raise NotImplementedError(f\"\"\"num_tokens_from_messages() is not implemented for model {model}. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\")\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: gpt-3.5-turbo may change over time. Returning num tokens assuming gpt-3.5-turbo-0301.\n",
      "127 prompt tokens counted.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "  {\"role\": \"system\", \"content\": \"You are a helpful, pattern-following assistant that translates corporate jargon into plain English.\"},\n",
    "  {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"New synergies will help drive top-line growth.\"},\n",
    "  {\"role\": \"system\", \"name\": \"example_assistant\", \"content\": \"Things working well together will increase revenue.\"},\n",
    "  {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\"},\n",
    "  {\"role\": \"system\", \"name\": \"example_assistant\", \"content\": \"Let's talk later when we're less busy about how to do better.\"},\n",
    "  {\"role\": \"user\", \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\"}\n",
    "]\n",
    "\n",
    "model = \"gpt-3.5-turbo\"\n",
    "\n",
    "print(f\"{num_tokens_from_messages(messages, model)} prompt tokens counted.\")\n",
    "# Should show ~126 total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject at 0x13ab8e0c0> JSON: {\n",
       "  \"completion_tokens\": 1,\n",
       "  \"prompt_tokens\": 127,\n",
       "  \"total_tokens\": 128\n",
       "}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.ChatCompletion.create(model=model, messages=messages, temperature=0, max_tokens=1)['usage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conversation:\n",
    "    def __init__(self, prompt):\n",
    "        self.model = \"gpt-3.5-turbo\"\n",
    "        self.prompt = prompt\n",
    "        self.messages = [{\"role\": \"system\", \"content\": self.prompt}]\n",
    "    \n",
    "    def ask(self, question, pprint=True):\n",
    "        self.messages.append({\"role\": \"user\", \"content\": question})\n",
    "\n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=self.model,\n",
    "                messages=self.messages\n",
    "            )\n",
    "        except Exception as e:\n",
    "            return e\n",
    "\n",
    "        if pprint:\n",
    "            print(f\"tiktoken: {self.num_tokens(self.messages, self.model)}\\ntokens: {response['usage']}\")\n",
    "        \n",
    "        assistant_message = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
    "\n",
    "        return assistant_message\n",
    "    \n",
    "    def num_tokens(self, messages, model=\"gpt-3.5-turbo-0301\"):\n",
    "        \"\"\"Returns the number of tokens used by a list of messages.\"\"\"\n",
    "        try:\n",
    "            encoding = tiktoken.encoding_for_model(model)\n",
    "        except KeyError:\n",
    "            print(\"Warning: model not found. Using cl100k_base encoding.\")\n",
    "            encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        if model == \"gpt-3.5-turbo\":\n",
    "            print(\"Warning: gpt-3.5-turbo may change over time. Returning num tokens assuming gpt-3.5-turbo-0301.\")\n",
    "            return self.num_tokens(messages, model=\"gpt-3.5-turbo-0301\")\n",
    "        elif model == \"gpt-4\":\n",
    "            print(\"Warning: gpt-4 may change over time. Returning num tokens assuming gpt-4-0314.\")\n",
    "            return self.num_tokens(messages, model=\"gpt-4-0314\")\n",
    "        elif model == \"gpt-3.5-turbo-0301\":\n",
    "            tokens_per_message = 4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n",
    "            tokens_per_name = -1  # if there's a name, the role is omitted\n",
    "        elif model == \"gpt-4-0314\":\n",
    "            tokens_per_message = 3\n",
    "            tokens_per_name = 1\n",
    "        else:\n",
    "            raise NotImplementedError(f\"\"\"num_tokens_from_messages() is not implemented for model {model}. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\")\n",
    "        num_tokens = 0\n",
    "        for message in messages:\n",
    "            num_tokens += tokens_per_message\n",
    "            for key, value in message.items():\n",
    "                num_tokens += len(encoding.encode(value))\n",
    "                if key == \"name\":\n",
    "                    num_tokens += tokens_per_name\n",
    "        num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
    "        return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: gpt-3.5-turbo may change over time. Returning num tokens assuming gpt-3.5-turbo-0301.\n",
      "tiktoken: 44\n",
      "tokens: {\n",
      "  \"completion_tokens\": 2,\n",
      "  \"prompt_tokens\": 44,\n",
      "  \"total_tokens\": 46\n",
      "}\n",
      "User: 你好！\n",
      "Assistant: Hello!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conv = Conversation(\"您是个翻译员，帮助用户把中文翻译成英文。\")\n",
    "translate(conv, \"你好！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: gpt-3.5-turbo may change over time. Returning num tokens assuming gpt-3.5-turbo-0301.\n",
      "tiktoken: 62\n",
      "tokens: {\n",
      "  \"completion_tokens\": 5,\n",
      "  \"prompt_tokens\": 62,\n",
      "  \"total_tokens\": 67\n",
      "}\n",
      "User: 你在哪里？\n",
      "Assistant: Where are you located?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "translate(conv, \"你在哪里？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: gpt-3.5-turbo may change over time. Returning num tokens assuming gpt-3.5-turbo-0301.\n",
      "tiktoken: 85\n",
      "tokens: {\n",
      "  \"completion_tokens\": 5,\n",
      "  \"prompt_tokens\": 85,\n",
      "  \"total_tokens\": 90\n",
      "}\n",
      "User: 你在干什么？\n",
      "Assistant: What are you doing?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "translate(conv, \"你在干什么？\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 插入文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "疑是地上霜，\n",
      "\n",
      "一夜凝碧万里层。\n"
     ]
    }
   ],
   "source": [
    "prefix = \"床前明月光，\\n\"\n",
    "suffix = \"\\n低头思故乡。\"\n",
    "response = openai.Completion.create(\n",
    "    model=\"text-davinci-003\",\n",
    "    prompt=f\"{prefix}{suffix}\",\n",
    "    max_tokens=64\n",
    ")\n",
    "print(response[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "疑是地上霜。\n",
      "\n",
      "举头望明月，\n",
      "\n",
      "低头思故乡。\n",
      "\n",
      "七言律诗\n",
      "\n",
      "Chuáng qián míng y\n"
     ]
    }
   ],
   "source": [
    "response = openai.Completion.create(\n",
    "    model=\"text-davinci-003\",\n",
    "    prompt=prefix,\n",
    "    suffix=suffix,\n",
    "    max_tokens=64\n",
    ")\n",
    "print(response[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "床前明月光，\n",
      "疑是地上霜。\n",
      "举头望明月，\n",
      "低头思故乡。\n"
     ]
    }
   ],
   "source": [
    "prefix = \"床前明月光，\\n\"\n",
    "suffix = \"低头思故乡。\"\n",
    "response = openai.Completion.create(\n",
    "    model=\"text-davinci-003\",\n",
    "    prompt=f\"{prefix}{suffix}\",\n",
    "    max_tokens=64\n",
    ")\n",
    "print(response[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "疑是地上霜；\n",
      "\n",
      "举头望明月，\n",
      "\n",
      "低头思故乡。\n",
      "\n",
      "Chuángqián míngyuè guāng,\n",
      "\n",
      "Yí sh\n"
     ]
    }
   ],
   "source": [
    "prefix = \"床前明月光，\\n\"\n",
    "suffix = \"低头思故乡。\"\n",
    "response = openai.Completion.create(\n",
    "    model=\"text-davinci-003\",\n",
    "    prompt=prefix,\n",
    "    suffix=suffix,\n",
    "    max_tokens=64\n",
    ")\n",
    "print(response[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "疑是地上霜。\n",
      "举头望明月，\n",
      "低头思故乡。\n",
      "\n",
      "Before my bed, the bright moonlight,\n",
      "I think it is frost on the ground.\n",
      "I looked up at\n"
     ]
    }
   ],
   "source": [
    "prefix = \"床前明月光，\"\n",
    "suffix = \"低头思故乡。\"\n",
    "response = openai.Completion.create(\n",
    "    model=\"text-davinci-003\",\n",
    "    prompt=prefix,\n",
    "    suffix=suffix,\n",
    "    max_tokens=64\n",
    ")\n",
    "print(response[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"length\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"text\": \"\\n\\u7591\\u662f\\u5730\\u4e0a\\u971c\\u3002\\n\\u4e3e\\u5934\\u671b\\u660e\\u6708\\uff0c\\n\\u4f4e\\u5934\\u601d\\u6545\\u4e61\\u3002\\n\\nChu\\u0101ng qi\\u00e1n m\\u00edngyu\\u00e8 gu\\u0101ng,\\nG\\u01d0 sh\\u00ec d\"\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1682826755,\n",
      "  \"id\": \"cmpl-7AsOxY1iuCqFIJeh1AE174k1dGWrk\",\n",
      "  \"model\": \"text-davinci-003\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 64,\n",
      "    \"prompt_tokens\": 23,\n",
      "    \"total_tokens\": 87\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "prefix = \"床前明月光，\"\n",
    "suffix = \"低头思故乡。\"\n",
    "response = openai.Completion.create(\n",
    "    model=\"text-davinci-003\",\n",
    "    prompt=prefix,\n",
    "    suffix=suffix,\n",
    "    max_tokens=64\n",
    ")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
